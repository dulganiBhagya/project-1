{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47fe7717-968a-4d0b-acee-85a65770f5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\BERAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\BERAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\BERAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BERAT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout,Conv1D, MaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b5ad1d5-1c3d-46c7-b4ef-491ef0a183dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unique_id>0785758968:one_of_the_best_crichton_novels:joseph_m</unique_id><asin>0785758968</asin><product_name>Sphere: Books: Michael Crichton</product_name><product_type>books</product_type><helpful>0 of 1</helpful><rating>5.0</rating><title>One of the best Crichton novels</title><date>July 1, 2006</date><reviewer>Joseph M</reviewer><reviewer_location>Colorado, USA</reviewer_location><review_text>Sphere by Michael Crichton is an excellant novel. This was certainly the hardest to put down of all of the Crichton novels that I have read.The story revolves around a man named Norman Johnson. Johnson is a phycologist. He travels with 4 other civilans to a remote location in the Pacific Ocean to help the Navy in a top secret misssion. They quickly learn that under the ocean is a half mile long spaceship. The civilans travel to a center 1000 feet under the ocean to live while researching the spacecraft. They are joined by 5 Navy personel to help them run operations. However on the surface a typhoon comes and the support ships on the surface must leave. The team of ten is stuck 1000 feet under the surface of the ocean. After a day under the sea they find out that the spacecraft is actually an American ship that has explored black holes and has brought back some strange things back to earth.This novel does not have the research that some of the other Crichton novels have, but it still has a lot of information on random things from the lawes of partial pressure to behavior analysis.I would strongly recommend this book</review_text>']\n"
     ]
    }
   ],
   "source": [
    "# This function extracts individual reviews from the raw lines of a review file.\n",
    "def extract_reviews_from_lines(lines):\n",
    "    reviews = []\n",
    "    review = []\n",
    "    inside_review = False\n",
    "    for line in lines:\n",
    "        if '<review>' in line:\n",
    "            inside_review = True\n",
    "        elif '</review>' in line:\n",
    "            inside_review = False\n",
    "            reviews.append(''.join(review).strip())  # Join all lines of a review into a single string\n",
    "            review = []  # Reset for the next review\n",
    "        elif inside_review:\n",
    "            review.append(line.strip())\n",
    "    return reviews\n",
    "\n",
    "# This function loads reviews for specified product types and labels.\n",
    "def load_reviews(product_types, labels):\n",
    "    data = {}\n",
    "\n",
    "    for product in product_types:\n",
    "        data[product] = {}\n",
    "\n",
    "        for label in labels:\n",
    "            file_path = f\"sorted_data_acl/{product}/{label}.review\"\n",
    "            try:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    lines = file.readlines()\n",
    "                    data[product][label] = extract_reviews_from_lines(lines)\n",
    "            except FileNotFoundError:\n",
    "                print(f\"File {file_path} not found. Skipping...\")\n",
    "                data[product][label] = []\n",
    "\n",
    "    return data\n",
    "\n",
    "product_types = ['books', 'dvd', 'electronics', 'kitchen_&_housewares']\n",
    "labels = ['negative', 'positive']\n",
    "\n",
    "# Load reviews for all product and label combinations.\n",
    "reviews_data = load_reviews(product_types, labels)\n",
    "\n",
    "# Flatten out negative and positive reviews from all product categories into two separate lists.\n",
    "all_negative_reviews = [review for product in product_types for review in reviews_data[product]['negative']]\n",
    "all_positive_reviews = [review for product in product_types for review in reviews_data[product]['positive']]\n",
    "\n",
    "print(all_positive_reviews[:1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cc02925-e3a8-44f1-a4bd-91a3d419266e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"THis book was horrible.  If it was possible to rate it lower than one star i would have.  I am an avid reader and picked this book up after my mom had gotten it from a friend.  I read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  One less copy in the world...don't waste your money.I wish i had the time spent reading this book back so i could use it for better purposes.  THis book wasted my life\"], ['I like to use the Amazon reviews when purchasing books, especially alert for dissenting perceptions about higly rated items, which usually disuades me from a selection.  So I offer this review that seriously questions the popularity of this work - I found it smug, self-serving and self-indulgent, written by a person with little or no empathy, especially for the people he castigates. For example, his portrayal of the family therapist seems implausible and reaches for effect and panders to the\"shrink\" bashers of the world. This \"play for effect\" tone throughout the book was very distasteful to me']]\n"
     ]
    }
   ],
   "source": [
    "# This function extracts the actual review text from the full review content.\n",
    "\n",
    "def extract_review_text(review):\n",
    "    pattern = r\"<review_text>(.*?)</review_text>\"\n",
    "    match = re.search(pattern, review, re.DOTALL) \n",
    "    return [match.group(1).strip()] if match else []\n",
    "\n",
    "# For each review in the all_negative(or positive)_reviews list, the actual review text is extracted \n",
    "# using the function above. If there's a valid extracted review, it's added to the list.\n",
    "all_negative_reviews = [extract_review_text(review) for review in all_negative_reviews if extract_review_text(review)]\n",
    "all_positive_reviews = [extract_review_text(review) for review in all_positive_reviews if extract_review_text(review)]\n",
    "\n",
    "print(all_negative_reviews[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "064ea765-a391-4701-b335-a861c6063c82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"this book was horrible.  if it was possible to rate it lower than one star i would have.  i am an avid reader and picked this book up after my mom had gotten it from a friend.  i read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  one less copy in the world...don't waste your money.i wish i had the time spent reading this book back so i could use it for better purposes.  this book wasted my life\"], ['i like to use the amazon reviews when purchasing books, especially alert for dissenting perceptions about higly rated items, which usually disuades me from a selection.  so i offer this review that seriously questions the popularity of this work - i found it smug, self-serving and self-indulgent, written by a person with little or no empathy, especially for the people he castigates. for example, his portrayal of the family therapist seems implausible and reaches for effect and panders to the\"shrink\" bashers of the world. this \"play for effect\" tone throughout the book was very distasteful to me']]\n"
     ]
    }
   ],
   "source": [
    "# Converting all reviews' letters to lowercase\n",
    "all_negative_reviews = [[text[0].lower()] for text in all_negative_reviews]\n",
    "all_positive_reviews = [[text[0].lower()] for text in all_positive_reviews]\n",
    "\n",
    "print(all_negative_reviews[:2]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da846e-a557-472e-8157-389f9a4b763b",
   "metadata": {},
   "source": [
    "# Cleaning and tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2855832-c530-4fb4-9386-f321857c76e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_review_text(text):\n",
    "\n",
    "    # Extract the review string from the list\n",
    "    review = text[0]\n",
    "\n",
    "    # Remove underscores and slashes\n",
    "    review = review.replace('_',' ').replace('/','')\n",
    "\n",
    "    # Remove numbers\n",
    "    review = re.sub(r'\\b[0-9]+\\b\\s*', '', review)\n",
    "\n",
    "    # Remove hyperlinks\n",
    "    review = re.sub(r'https?://\\S+', '', review)\n",
    "\n",
    "    # Remove the <a> tags but keep their contents\n",
    "    review = re.sub(r'<a[^>]*>(.*?)</a>', r'\\1', review)\n",
    "\n",
    "    # Remove other HTML tags but keep their contents\n",
    "    review = re.sub(r'<.*?>', '', review)\n",
    "\n",
    "    # Remove alphanumerics that don't mean anything\n",
    "    review = re.sub(r'\\w*\\d\\w*', '', review)\n",
    "\n",
    "    # Tokenize without undesired punctuation and return as a list of tokens\n",
    "    tokens = RegexpTokenizer(r\"\\w+(?:'\\w+)?|!|\\?\").tokenize(review)\n",
    "\n",
    "    return tokens  # We return a list of lists (with the tokens as a sub-list)\n",
    "\n",
    "# Apply the function to each review in the lists of lists\n",
    "cleaned_negative_reviews = [clean_review_text(review) for review in all_negative_reviews]\n",
    "cleaned_positive_reviews = [clean_review_text(review) for review in all_positive_reviews]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574c36b-b22e-427c-b519-08c22d3ec507",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c368528-fefc-4592-872a-e8bf101e2edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "retain_words = {'not', 'no', \"isn't\", \"hasn't\", \"wasn't\", \"don't\", \"doesn't\", \"didn't\",\n",
    "                \"can't\", \"couldn't\", \"won't\", \"wouldn't\", \"shouldn't\", \"hardly\", \"just\",\n",
    "                \"only\", \"always\", \"never\", \"could\"}\n",
    "\n",
    "# Remove retained words from the set of stopwords\n",
    "stop_words = stop_words - retain_words\n",
    "\n",
    "# This function takes in a review and removes any word in it that's part of the stopwords. \n",
    "def remove_stopwords(review):\n",
    "    return [[word for word in tokens if word not in stop_words] for tokens in review]\n",
    "\n",
    "# Remove stopwords from the tokenized reviews\n",
    "cleaned_negative_reviews = remove_stopwords(cleaned_negative_reviews)\n",
    "cleaned_positive_reviews = remove_stopwords(cleaned_positive_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9c3c0-6691-46a9-a0af-f261f93454fe",
   "metadata": {},
   "source": [
    "# Chunking and POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ddaa10b-cc49-48c8-8ade-c55679005b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noun Phrase (NP)\n",
    "# Verb Phrase (VP)\n",
    "# Adjective Phrase (ADJP) (there are 2 patterns we can use {<RB>?<JJ> and <JJ><NN|JJ>?}.\n",
    "# I am using both.\n",
    "# Adverb Phrase (ADVP)\n",
    "# Prepositional Phrase (PP)\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ.*>*<NN.*>+} \n",
    "    VP: {<VB.*><NP|JJ|PP|ADVP|VB>*}\n",
    "    ADJP: {<RB>?<JJ>|<JJ><NN|JJ>?}\n",
    "    ADVP: {<RB.*><RB>?}\n",
    "    PP: {<IN><NP>} \n",
    "\"\"\"\n",
    "\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "# Function to POS tag each review\n",
    "def pos_tag_reviews(reviews):\n",
    "    return [nltk.pos_tag(review) for review in reviews]\n",
    "\n",
    "# Function to chunk each tagged review\n",
    "def chunk_reviews(tagged_reviews):\n",
    "    return [chunk_parser.parse(tagged_review) for tagged_review in tagged_reviews]\n",
    "\n",
    "# POS tag the reviews\n",
    "tagged_negative_reviews = pos_tag_reviews(cleaned_negative_reviews)\n",
    "tagged_positive_reviews = pos_tag_reviews(cleaned_positive_reviews)\n",
    "\n",
    "# Chunk the tagged reviews\n",
    "chunked_negative_reviews = chunk_reviews(tagged_negative_reviews)\n",
    "chunked_positive_reviews = chunk_reviews(tagged_positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80c3da0-2452-46c2-bbde-2980882ac28f",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8861ff4b-5fad-46af-b21f-a875651dd7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map treebank POS tag to first character used by WordNetLemmatizer.\"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize words in a tagged review using their respective POS tag.\n",
    "def lemmatize_review(tagged_review):\n",
    "    return [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_review]\n",
    "\n",
    "# Function to lemmatize each review in the list of reviews\n",
    "def lemmatize_reviews(tagged_reviews):\n",
    "    return [lemmatize_review(review) for review in tagged_reviews]\n",
    "\n",
    "# Apply the lemmatization function to the tagged reviews\n",
    "lemmatized_negative_reviews = lemmatize_reviews(tagged_negative_reviews)\n",
    "lemmatized_positive_reviews = lemmatize_reviews(tagged_positive_reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4997f5-9369-4b6c-8083-0c3c8007c795",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09118393-2b4a-4b12-a4c0-76bfb7445677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flatten the reviews to create all_words\n",
    "all_words = [word for review in (lemmatized_negative_reviews + lemmatized_positive_reviews) for word in review]\n",
    "\n",
    "# Calculate the frequency distribution\n",
    "fdist = FreqDist(all_words)\n",
    "\n",
    "# Create word-to-int mapping\n",
    "sorted_vocab = sorted(fdist, key=fdist.get, reverse=True)\n",
    "word2int = {word: i+1 for i, word in enumerate(sorted_vocab)}  # start from 1 to reserve 0 for padding later\n",
    "\n",
    "# Function to encode a single review\n",
    "def encode_review(review):\n",
    "    return [word2int[word] for word in review if word in word2int]\n",
    "\n",
    "# Encode the reviews while maintaining the list of lists structure\n",
    "encoded_negative_reviews = [encode_review(review) for review in lemmatized_negative_reviews]\n",
    "encoded_positive_reviews = [encode_review(review) for review in lemmatized_positive_reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14b0207d-ac93-4aa6-b9d3-4c17588074ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('not', 6972), ('book', 5242), ('one', 5145), ('!', 4365), ('get', 4030), ('use', 3625), ('like', 3402), ('make', 3376), ('just', 3211), ('good', 3110)]\n"
     ]
    }
   ],
   "source": [
    "# print the most common words\n",
    "print(fdist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feb6c1a6-0d6c-4ffe-bc4b-9c45023b0451",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'book': 5242, 'horrible': 155, 'possible': 140, 'rate': 148, 'lower': 21, 'one': 5145, 'star': 744, 'would': 2759, 'avid': 22, 'reader': 435}\n"
     ]
    }
   ],
   "source": [
    "word_freq = dict(fdist)\n",
    "\n",
    "# for top 10 words\n",
    "n = 10  \n",
    "top_words = {k: word_freq[k] for k in list(word_freq)[:n]}\n",
    "print(top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4fdf88-24ad-4812-a505-207beee80c85",
   "metadata": {},
   "source": [
    "# Encode the labels for ‘positive’ and ‘negative’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cd6007b-5052-487b-893c-a703a64287a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating labels for positive and negative reviews. \n",
    "# For our sentiment analysis, we're using binary classification: \n",
    "# 1 represents positive sentiments, and 0 represents negative sentiments.\n",
    "\n",
    "# Generate a list of ones with the same length as `lemmatized_positive_reviews`\n",
    "labels_positive = [1] * len(lemmatized_positive_reviews)\n",
    "\n",
    "# Generate a list of zeros with the same length as `lemmatized_negative_reviews`\n",
    "labels_negative = [0] * len(lemmatized_negative_reviews)\n",
    "\n",
    "# Concatenate the two lists to get a complete list of labels for both positive and negative reviews\n",
    "all_labels = labels_negative + labels_positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e22c1a-0acb-409f-a8e0-9b0e210a7caa",
   "metadata": {},
   "source": [
    "# outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20c10226-d2fa-4e0b-9b39-c97062f2160d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the length of each review in the combined list of both lemmatized positive and negative reviews\n",
    "review_lengths = [len(review) for review in lemmatized_negative_reviews + lemmatized_positive_reviews]\n",
    "\n",
    "# Compute the average (mean) length of all reviews\n",
    "avg_length = np.mean(review_lengths)\n",
    "\n",
    "# Compute the standard deviation of the lengths of all reviews. \n",
    "std_length = np.std(review_lengths)\n",
    "\n",
    "\n",
    "# Define a minimum threshold for review length as one standard deviation below the average length. \n",
    "min_threshold = avg_length - std_length  # 1 standard deviation below the mean\n",
    "\n",
    "# Define a maximum threshold for review length as two standard deviations above the average length. \n",
    "max_threshold = avg_length + 2 * std_length  # 2 standard deviations above the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c68af42-21c2-48a4-8de8-40577fe1907d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of reviews that only includes those whose lengths are between the defined min_threshold and max_threshold.\n",
    "# It filters out reviews that are too short (less than min_threshold) or too long (greater than max_threshold).\n",
    "filtered_reviews = [review for review in lemmatized_negative_reviews + lemmatized_positive_reviews if min_threshold <= len(review) <= max_threshold]\n",
    "\n",
    "# Similarly, for labels corresponding to the reviews, we only want to keep those that are associated with reviews \n",
    "# whose lengths are between the defined min_threshold and max_threshold.\n",
    "filtered_labels = [label for review, label in zip(lemmatized_negative_reviews + lemmatized_positive_reviews, all_labels) if min_threshold <= len(review) <= max_threshold]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05deef0b-8678-44cd-9c46-bfce5817694d",
   "metadata": {},
   "source": [
    "# pad/truncate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4446db93-1d39-4131-ae1e-e3919dff337e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, we're filtering encoded reviews based on their length to remove those that are too short or too long.\n",
    "# The result is a list of reviews whose lengths are between min_threshold and max_threshold.\n",
    "filtered_encoded_reviews = [review for review in encoded_negative_reviews + encoded_positive_reviews if min_threshold <= len(review) <= max_threshold]\n",
    "\n",
    "# Similarly, we filter the labels of the reviews to ensure they correspond to the lengths of the reviews \n",
    "# that we're keeping after the above filtering process.\n",
    "filtered_labels = [label for review, label in zip(encoded_negative_reviews + encoded_positive_reviews, all_labels) if min_threshold <= len(review) <= max_threshold]\n",
    "\n",
    "# max_sequence_length is set based on max_threshold, which defines the maximum length for a review.\n",
    "# This value is used for padding and truncating reviews to ensure they all have the same length.\n",
    "max_sequence_length = int(max_threshold)  \n",
    "\n",
    "# Here, we're padding (or truncating) each review in our filtered list to make sure every review has the same length.\n",
    "# If a review is shorter than max_sequence_length, it gets padded with zeros at the end. \n",
    "# If a review is longer than max_sequence_length, it's truncated to fit the specified length.\n",
    "padded_reviews = pad_sequences(filtered_encoded_reviews, maxlen=max_sequence_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047c07e-8b7e-4468-89c3-8c51c362323f",
   "metadata": {},
   "source": [
    "# split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76db3f34-8bfc-42de-94b1-5d0c3244ba88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, split data into training and temp (validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded_reviews, filtered_labels, test_size=0.2, stratify=filtered_labels, random_state=42)\n",
    "\n",
    "# Then, split temp data into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc599f8-2c9d-43fb-b38d-e36c66295c36",
   "metadata": {},
   "source": [
    "# building the ann model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b504ade-b5d0-49d1-9d10-c59dea048b67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 236, 64)           2217792   \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 15104)             0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                966720    \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3184577 (12.15 MB)\n",
      "Trainable params: 3184577 (12.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2int) + 1  \n",
    "embedding_dim = 64  \n",
    "max_length = int(max_threshold)  \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer to convert integer sequences to dense vectors of fixed size.\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "\n",
    "# Flatten the 2D embedding output to 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "# Hidden layer with 64 units and ReLU activation\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer with 1 unit (binary classification: positive or negative)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model using binary cross-entropy loss, since it's a binary classification problem\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03d18b02-7227-4a59-80fc-717b256b3a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "192/192 [==============================] - 11s 51ms/step - loss: 0.6423 - accuracy: 0.6070 - val_loss: 0.4665 - val_accuracy: 0.7804\n",
      "Epoch 2/5\n",
      "192/192 [==============================] - 10s 51ms/step - loss: 0.2574 - accuracy: 0.9000 - val_loss: 0.4440 - val_accuracy: 0.8157\n",
      "Epoch 3/5\n",
      "192/192 [==============================] - 10s 52ms/step - loss: 0.0420 - accuracy: 0.9902 - val_loss: 0.5288 - val_accuracy: 0.8092\n",
      "Epoch 4/5\n",
      "192/192 [==============================] - 10s 53ms/step - loss: 0.0094 - accuracy: 0.9989 - val_loss: 0.6191 - val_accuracy: 0.8000\n",
      "Epoch 5/5\n",
      "192/192 [==============================] - 10s 50ms/step - loss: 0.0036 - accuracy: 0.9998 - val_loss: 0.6829 - val_accuracy: 0.7987\n"
     ]
    }
   ],
   "source": [
    "# Convert y_train and y_val to numpy arrays.\n",
    "# Neural networks in TensorFlow/Keras expect the input data and labels to be numpy arrays.\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "# Fit the model using the training data.\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e1459fbd-439b-4031-835c-fe8a57eac2c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 4ms/step - loss: 0.7021 - accuracy: 0.7911\n",
      "Test Accuracy: 79.11%\n"
     ]
    }
   ],
   "source": [
    "# Convert y_test to a numpy array.\n",
    "# Neural networks in TensorFlow/Keras expect the labels to be numpy arrays.\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Evaluate the performance of the trained model on the test set.\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print the accuracy of the model on the test set.\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e972cc0-6a9a-45b7-ba78-741d31aa5b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BERAT\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model for the website\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fba151a-37d3-4b1d-a770-39529960e46c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Variables to save for the website\n",
    "variables = {\n",
    "    \"max_length\": max_length,\n",
    "    \"word2int\": word2int\n",
    "}\n",
    "\n",
    "# Save the variables to a pickle file\n",
    "with open('variables.pkl', 'wb') as f:\n",
    "    pickle.dump(variables, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
